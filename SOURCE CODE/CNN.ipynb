{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision.datasets import CIFAR10\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\nimport torchvision\nfrom torchsummary import summary\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ntorch.manual_seed(1)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n\nbatch_size = 64\n\ntrain_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2470, 0.2435, 0.2616]),\n])\n\nval_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2470, 0.2435, 0.2616]),\n])\n\ntrain_set = CIFAR10(root='./data', train=True, download=True, transform=train_transform)\nval_set = CIFAR10(root='./data', train=False, download=True, transform=val_transform)\n\ntrainloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\ntestloader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2)\n\nclass Inception(nn.Module):\n    def __init__(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):\n        super(Inception, self).__init__()\n        self.b1 = nn.Sequential(\n            nn.Conv2d(in_planes, n1x1, kernel_size=1),\n            nn.BatchNorm2d(n1x1),\n            nn.ReLU(True),\n        )\n        self.b2 = nn.Sequential(\n            nn.Conv2d(in_planes, n3x3red, kernel_size=1),\n            nn.BatchNorm2d(n3x3red),\n            nn.ReLU(True),\n            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n3x3),\n            nn.ReLU(True),\n        )\n        self.b3 = nn.Sequential(\n            nn.Conv2d(in_planes, n5x5red, kernel_size=1),\n            nn.BatchNorm2d(n5x5red),\n            nn.ReLU(True),\n            nn.Conv2d(n5x5red, n5x5, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n5x5),\n            nn.ReLU(True),\n            nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n5x5),\n            nn.ReLU(True),\n        )\n        self.b4 = nn.Sequential(\n            nn.MaxPool2d(3, stride=1, padding=1),\n            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n            nn.BatchNorm2d(pool_planes),\n            nn.ReLU(True),\n        )\n\n    def forward(self, x):\n        y1 = self.b1(x)\n        y2 = self.b2(x)\n        y3 = self.b3(x)\n        y4 = self.b4(x)\n        return torch.cat([y1, y2, y3, y4], 1)\n\nclass GoogLeNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(GoogLeNet, self).__init__()\n        self.pre_layers = nn.Sequential(\n            nn.Conv2d(3, 192, kernel_size=3, padding=1),\n            nn.BatchNorm2d(192),\n            nn.ReLU(True),\n        )\n        self.a3 = Inception(192, 64, 96, 128, 16, 32, 32)\n        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n        self.a4 = Inception(480, 192, 96, 208, 16, 48, 64)\n        self.b4 = Inception(512, 160, 112, 224, 24, 64, 64)\n        self.c4 = Inception(512, 128, 128, 256, 24, 64, 64)\n        self.d4 = Inception(512, 112, 144, 288, 32, 64, 64)\n        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n        self.avgpool = nn.AvgPool2d(8, stride=1)\n        self.linear = nn.Linear(1024, num_classes)\n\n    def forward(self, x):\n        out = self.pre_layers(x)\n        out = self.a3(out)\n        out = self.b3(out)\n        out = self.maxpool(out)\n        out = self.a4(out)\n        out = self.b4(out)\n        out = self.c4(out)\n        out = self.d4(out)\n        out = self.e4(out)\n        out = self.maxpool(out)\n        out = self.a5(out)\n        out = self.b5(out)\n        out = self.avgpool(out)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\nmodel = GoogLeNet(num_classes=10).to(device)\nsummary(model, (3, 32, 32))\ncriterion = nn.CrossEntropyLoss()\noptimizer = Adam(model.parameters(), lr=1e-3)\n\ndef evaluate(model, testloader, criterion):\n    model.eval()\n    test_loss = 0.0\n    running_correct = 0\n    total = 0\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for images, labels in testloader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            running_correct += (predicted == labels).sum().item()\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    accuracy = 100 * running_correct / total\n    test_loss = test_loss / len(testloader)\n    return test_loss, accuracy, all_preds, all_labels\n\ntrain_losses = []\ntrain_accuracies = []\ntest_losses = []\ntest_accuracies = []\nmax_epoch = 120\n\nfor epoch in range(max_epoch):\n    model.train()\n    running_loss = 0.0\n    running_correct = 0\n    total = 0\n    for i, (inputs, labels) in enumerate(trainloader, 0):\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        running_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        running_correct += (predicted == labels).sum().item()\n\n    epoch_accuracy = 100 * running_correct / total\n    epoch_loss = running_loss / (i + 1)\n    test_loss, test_accuracy, preds, labels = evaluate(model, testloader, criterion)\n    print(f\"Epoch [{epoch + 1:3}/{max_epoch:3}] \\t Loss: {epoch_loss:<11.5f} Accuracy: {epoch_accuracy:.2f}% \\t Test Loss: {test_loss:<11.5f} Test Accuracy: {test_accuracy:.2f}%\")\n\n    train_losses.append(epoch_loss)\n    train_accuracies.append(epoch_accuracy)\n    test_losses.append(test_loss)\n    test_accuracies.append(test_accuracy)\n\n# Plot training and test metrics\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='train_loss')\nplt.plot(test_losses, label='test_loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.subplot(1, 2, 2)\nplt.plot(train_accuracies, label='train_accuracy')\nplt.plot(test_accuracies, label='test_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy (%)')\nplt.legend()\nplt.show()\n\n# Compute and plot confusion matrix\ncm = confusion_matrix(labels, preds)\nclass_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T04:03:35.286039Z","iopub.execute_input":"2025-06-04T04:03:35.286243Z","execution_failed":"2025-06-04T04:09:40.174Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 170M/170M [00:01<00:00, 93.2MB/s] \n","output_type":"stream"},{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1          [-1, 192, 32, 32]           5,376\n       BatchNorm2d-2          [-1, 192, 32, 32]             384\n              ReLU-3          [-1, 192, 32, 32]               0\n            Conv2d-4           [-1, 64, 32, 32]          12,352\n       BatchNorm2d-5           [-1, 64, 32, 32]             128\n              ReLU-6           [-1, 64, 32, 32]               0\n            Conv2d-7           [-1, 96, 32, 32]          18,528\n       BatchNorm2d-8           [-1, 96, 32, 32]             192\n              ReLU-9           [-1, 96, 32, 32]               0\n           Conv2d-10          [-1, 128, 32, 32]         110,720\n      BatchNorm2d-11          [-1, 128, 32, 32]             256\n             ReLU-12          [-1, 128, 32, 32]               0\n           Conv2d-13           [-1, 16, 32, 32]           3,088\n      BatchNorm2d-14           [-1, 16, 32, 32]              32\n             ReLU-15           [-1, 16, 32, 32]               0\n           Conv2d-16           [-1, 32, 32, 32]           4,640\n      BatchNorm2d-17           [-1, 32, 32, 32]              64\n             ReLU-18           [-1, 32, 32, 32]               0\n           Conv2d-19           [-1, 32, 32, 32]           9,248\n      BatchNorm2d-20           [-1, 32, 32, 32]              64\n             ReLU-21           [-1, 32, 32, 32]               0\n        MaxPool2d-22          [-1, 192, 32, 32]               0\n           Conv2d-23           [-1, 32, 32, 32]           6,176\n      BatchNorm2d-24           [-1, 32, 32, 32]              64\n             ReLU-25           [-1, 32, 32, 32]               0\n        Inception-26          [-1, 256, 32, 32]               0\n           Conv2d-27          [-1, 128, 32, 32]          32,896\n      BatchNorm2d-28          [-1, 128, 32, 32]             256\n             ReLU-29          [-1, 128, 32, 32]               0\n           Conv2d-30          [-1, 128, 32, 32]          32,896\n      BatchNorm2d-31          [-1, 128, 32, 32]             256\n             ReLU-32          [-1, 128, 32, 32]               0\n           Conv2d-33          [-1, 192, 32, 32]         221,376\n      BatchNorm2d-34          [-1, 192, 32, 32]             384\n             ReLU-35          [-1, 192, 32, 32]               0\n           Conv2d-36           [-1, 32, 32, 32]           8,224\n      BatchNorm2d-37           [-1, 32, 32, 32]              64\n             ReLU-38           [-1, 32, 32, 32]               0\n           Conv2d-39           [-1, 96, 32, 32]          27,744\n      BatchNorm2d-40           [-1, 96, 32, 32]             192\n             ReLU-41           [-1, 96, 32, 32]               0\n           Conv2d-42           [-1, 96, 32, 32]          83,040\n      BatchNorm2d-43           [-1, 96, 32, 32]             192\n             ReLU-44           [-1, 96, 32, 32]               0\n        MaxPool2d-45          [-1, 256, 32, 32]               0\n           Conv2d-46           [-1, 64, 32, 32]          16,448\n      BatchNorm2d-47           [-1, 64, 32, 32]             128\n             ReLU-48           [-1, 64, 32, 32]               0\n        Inception-49          [-1, 480, 32, 32]               0\n        MaxPool2d-50          [-1, 480, 16, 16]               0\n           Conv2d-51          [-1, 192, 16, 16]          92,352\n      BatchNorm2d-52          [-1, 192, 16, 16]             384\n             ReLU-53          [-1, 192, 16, 16]               0\n           Conv2d-54           [-1, 96, 16, 16]          46,176\n      BatchNorm2d-55           [-1, 96, 16, 16]             192\n             ReLU-56           [-1, 96, 16, 16]               0\n           Conv2d-57          [-1, 208, 16, 16]         179,920\n      BatchNorm2d-58          [-1, 208, 16, 16]             416\n             ReLU-59          [-1, 208, 16, 16]               0\n           Conv2d-60           [-1, 16, 16, 16]           7,696\n      BatchNorm2d-61           [-1, 16, 16, 16]              32\n             ReLU-62           [-1, 16, 16, 16]               0\n           Conv2d-63           [-1, 48, 16, 16]           6,960\n      BatchNorm2d-64           [-1, 48, 16, 16]              96\n             ReLU-65           [-1, 48, 16, 16]               0\n           Conv2d-66           [-1, 48, 16, 16]          20,784\n      BatchNorm2d-67           [-1, 48, 16, 16]              96\n             ReLU-68           [-1, 48, 16, 16]               0\n        MaxPool2d-69          [-1, 480, 16, 16]               0\n           Conv2d-70           [-1, 64, 16, 16]          30,784\n      BatchNorm2d-71           [-1, 64, 16, 16]             128\n             ReLU-72           [-1, 64, 16, 16]               0\n        Inception-73          [-1, 512, 16, 16]               0\n           Conv2d-74          [-1, 160, 16, 16]          82,080\n      BatchNorm2d-75          [-1, 160, 16, 16]             320\n             ReLU-76          [-1, 160, 16, 16]               0\n           Conv2d-77          [-1, 112, 16, 16]          57,456\n      BatchNorm2d-78          [-1, 112, 16, 16]             224\n             ReLU-79          [-1, 112, 16, 16]               0\n           Conv2d-80          [-1, 224, 16, 16]         226,016\n      BatchNorm2d-81          [-1, 224, 16, 16]             448\n             ReLU-82          [-1, 224, 16, 16]               0\n           Conv2d-83           [-1, 24, 16, 16]          12,312\n      BatchNorm2d-84           [-1, 24, 16, 16]              48\n             ReLU-85           [-1, 24, 16, 16]               0\n           Conv2d-86           [-1, 64, 16, 16]          13,888\n      BatchNorm2d-87           [-1, 64, 16, 16]             128\n             ReLU-88           [-1, 64, 16, 16]               0\n           Conv2d-89           [-1, 64, 16, 16]          36,928\n      BatchNorm2d-90           [-1, 64, 16, 16]             128\n             ReLU-91           [-1, 64, 16, 16]               0\n        MaxPool2d-92          [-1, 512, 16, 16]               0\n           Conv2d-93           [-1, 64, 16, 16]          32,832\n      BatchNorm2d-94           [-1, 64, 16, 16]             128\n             ReLU-95           [-1, 64, 16, 16]               0\n        Inception-96          [-1, 512, 16, 16]               0\n           Conv2d-97          [-1, 128, 16, 16]          65,664\n      BatchNorm2d-98          [-1, 128, 16, 16]             256\n             ReLU-99          [-1, 128, 16, 16]               0\n          Conv2d-100          [-1, 128, 16, 16]          65,664\n     BatchNorm2d-101          [-1, 128, 16, 16]             256\n            ReLU-102          [-1, 128, 16, 16]               0\n          Conv2d-103          [-1, 256, 16, 16]         295,168\n     BatchNorm2d-104          [-1, 256, 16, 16]             512\n            ReLU-105          [-1, 256, 16, 16]               0\n          Conv2d-106           [-1, 24, 16, 16]          12,312\n     BatchNorm2d-107           [-1, 24, 16, 16]              48\n            ReLU-108           [-1, 24, 16, 16]               0\n          Conv2d-109           [-1, 64, 16, 16]          13,888\n     BatchNorm2d-110           [-1, 64, 16, 16]             128\n            ReLU-111           [-1, 64, 16, 16]               0\n          Conv2d-112           [-1, 64, 16, 16]          36,928\n     BatchNorm2d-113           [-1, 64, 16, 16]             128\n            ReLU-114           [-1, 64, 16, 16]               0\n       MaxPool2d-115          [-1, 512, 16, 16]               0\n          Conv2d-116           [-1, 64, 16, 16]          32,832\n     BatchNorm2d-117           [-1, 64, 16, 16]             128\n            ReLU-118           [-1, 64, 16, 16]               0\n       Inception-119          [-1, 512, 16, 16]               0\n          Conv2d-120          [-1, 112, 16, 16]          57,456\n     BatchNorm2d-121          [-1, 112, 16, 16]             224\n            ReLU-122          [-1, 112, 16, 16]               0\n          Conv2d-123          [-1, 144, 16, 16]          73,872\n     BatchNorm2d-124          [-1, 144, 16, 16]             288\n            ReLU-125          [-1, 144, 16, 16]               0\n          Conv2d-126          [-1, 288, 16, 16]         373,536\n     BatchNorm2d-127          [-1, 288, 16, 16]             576\n            ReLU-128          [-1, 288, 16, 16]               0\n          Conv2d-129           [-1, 32, 16, 16]          16,416\n     BatchNorm2d-130           [-1, 32, 16, 16]              64\n            ReLU-131           [-1, 32, 16, 16]               0\n          Conv2d-132           [-1, 64, 16, 16]          18,496\n     BatchNorm2d-133           [-1, 64, 16, 16]             128\n            ReLU-134           [-1, 64, 16, 16]               0\n          Conv2d-135           [-1, 64, 16, 16]          36,928\n     BatchNorm2d-136           [-1, 64, 16, 16]             128\n            ReLU-137           [-1, 64, 16, 16]               0\n       MaxPool2d-138          [-1, 512, 16, 16]               0\n          Conv2d-139           [-1, 64, 16, 16]          32,832\n     BatchNorm2d-140           [-1, 64, 16, 16]             128\n            ReLU-141           [-1, 64, 16, 16]               0\n       Inception-142          [-1, 528, 16, 16]               0\n          Conv2d-143          [-1, 256, 16, 16]         135,424\n     BatchNorm2d-144          [-1, 256, 16, 16]             512\n            ReLU-145          [-1, 256, 16, 16]               0\n          Conv2d-146          [-1, 160, 16, 16]          84,640\n     BatchNorm2d-147          [-1, 160, 16, 16]             320\n            ReLU-148          [-1, 160, 16, 16]               0\n          Conv2d-149          [-1, 320, 16, 16]         461,120\n     BatchNorm2d-150          [-1, 320, 16, 16]             640\n            ReLU-151          [-1, 320, 16, 16]               0\n          Conv2d-152           [-1, 32, 16, 16]          16,928\n     BatchNorm2d-153           [-1, 32, 16, 16]              64\n            ReLU-154           [-1, 32, 16, 16]               0\n          Conv2d-155          [-1, 128, 16, 16]          36,992\n     BatchNorm2d-156          [-1, 128, 16, 16]             256\n            ReLU-157          [-1, 128, 16, 16]               0\n          Conv2d-158          [-1, 128, 16, 16]         147,584\n     BatchNorm2d-159          [-1, 128, 16, 16]             256\n            ReLU-160          [-1, 128, 16, 16]               0\n       MaxPool2d-161          [-1, 528, 16, 16]               0\n          Conv2d-162          [-1, 128, 16, 16]          67,712\n     BatchNorm2d-163          [-1, 128, 16, 16]             256\n            ReLU-164          [-1, 128, 16, 16]               0\n       Inception-165          [-1, 832, 16, 16]               0\n       MaxPool2d-166            [-1, 832, 8, 8]               0\n          Conv2d-167            [-1, 256, 8, 8]         213,248\n     BatchNorm2d-168            [-1, 256, 8, 8]             512\n            ReLU-169            [-1, 256, 8, 8]               0\n          Conv2d-170            [-1, 160, 8, 8]         133,280\n     BatchNorm2d-171            [-1, 160, 8, 8]             320\n            ReLU-172            [-1, 160, 8, 8]               0\n          Conv2d-173            [-1, 320, 8, 8]         461,120\n     BatchNorm2d-174            [-1, 320, 8, 8]             640\n            ReLU-175            [-1, 320, 8, 8]               0\n          Conv2d-176             [-1, 32, 8, 8]          26,656\n     BatchNorm2d-177             [-1, 32, 8, 8]              64\n            ReLU-178             [-1, 32, 8, 8]               0\n          Conv2d-179            [-1, 128, 8, 8]          36,992\n     BatchNorm2d-180            [-1, 128, 8, 8]             256\n            ReLU-181            [-1, 128, 8, 8]               0\n          Conv2d-182            [-1, 128, 8, 8]         147,584\n     BatchNorm2d-183            [-1, 128, 8, 8]             256\n            ReLU-184            [-1, 128, 8, 8]               0\n       MaxPool2d-185            [-1, 832, 8, 8]               0\n          Conv2d-186            [-1, 128, 8, 8]         106,624\n     BatchNorm2d-187            [-1, 128, 8, 8]             256\n            ReLU-188            [-1, 128, 8, 8]               0\n       Inception-189            [-1, 832, 8, 8]               0\n          Conv2d-190            [-1, 384, 8, 8]         319,872\n     BatchNorm2d-191            [-1, 384, 8, 8]             768\n            ReLU-192            [-1, 384, 8, 8]               0\n          Conv2d-193            [-1, 192, 8, 8]         159,936\n     BatchNorm2d-194            [-1, 192, 8, 8]             384\n            ReLU-195            [-1, 192, 8, 8]               0\n          Conv2d-196            [-1, 384, 8, 8]         663,936\n     BatchNorm2d-197            [-1, 384, 8, 8]             768\n            ReLU-198            [-1, 384, 8, 8]               0\n          Conv2d-199             [-1, 48, 8, 8]          39,984\n     BatchNorm2d-200             [-1, 48, 8, 8]              96\n            ReLU-201             [-1, 48, 8, 8]               0\n          Conv2d-202            [-1, 128, 8, 8]          55,424\n     BatchNorm2d-203            [-1, 128, 8, 8]             256\n            ReLU-204            [-1, 128, 8, 8]               0\n          Conv2d-205            [-1, 128, 8, 8]         147,584\n     BatchNorm2d-206            [-1, 128, 8, 8]             256\n            ReLU-207            [-1, 128, 8, 8]               0\n       MaxPool2d-208            [-1, 832, 8, 8]               0\n          Conv2d-209            [-1, 128, 8, 8]         106,624\n     BatchNorm2d-210            [-1, 128, 8, 8]             256\n            ReLU-211            [-1, 128, 8, 8]               0\n       Inception-212           [-1, 1024, 8, 8]               0\n       AvgPool2d-213           [-1, 1024, 1, 1]               0\n          Linear-214                   [-1, 10]          10,250\n================================================================\nTotal params: 6,166,250\nTrainable params: 6,166,250\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.01\nForward/backward pass size (MB): 81.42\nParams size (MB): 23.52\nEstimated Total Size (MB): 104.96\n----------------------------------------------------------------\nEpoch [  1/  2] \t Loss: 1.22904     Accuracy: 55.08% \t Test Loss: 1.00988     Test Accuracy: 65.05%\n","output_type":"stream"}],"execution_count":null}]}